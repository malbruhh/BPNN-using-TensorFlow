{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8bc5ab",
   "metadata": {},
   "source": [
    "## 2nd Model : 2 Hidden Layers\n",
    "- Network Architecture: \n",
    "    - 2 hidden layers\n",
    "    - ReLU Activation on Hidden\n",
    "    - Sigmoid Activation on Output\n",
    "    - learning rate = 0.01\n",
    "    - epoch = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1ab9c",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "1. Import Libraries \n",
    "2. Read Data from `\\Dataset` folder (Retrieved from UCI Repositories)\n",
    "3. Clean Dataset (rename and format the column)\n",
    "4. Data Preprocessing\n",
    "5. Initialize the model\n",
    "6. Train and test the data\n",
    "7. Evaluate Performance using graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa48bd7",
   "metadata": {},
   "source": [
    "### 0. Install Library Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install jupyter numpy pandas matplotlib seaborn tabulate imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b92ecb",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "Import necessary libraries:\n",
    "* `numpy`     : High performance array object\n",
    "* `pandas`    : Data analysis and manipulation tools\n",
    "* `matplotlib`: Data visualizations\n",
    "* `seaborn`   : Data visualizations framework based on **matplotlib**\n",
    "* `tabulate`  : Pretty-print tabular structure library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1323f",
   "metadata": {},
   "source": [
    "#### 1.1 Helper Functions\n",
    "* Handle mathematical operation used in the `NeuralNetwork` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 /(1 + np.exp(-x))\n",
    "def relu(x): return np.maximum(0,x)\n",
    "def derivative_relu(relu_x): return (relu_x>0).astype(float)\n",
    "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "    # Clip predictions to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0b083",
   "metadata": {},
   "source": [
    "### 2. Read and Display Dataset Information\n",
    "load the dataset and understand the data, and use the Interquartile Range (IQR) method to identify potential outliers in numerical columns.\n",
    "2.1 Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str):\n",
    "    df = pd.read_csv(file_path)\n",
    "    new_columns = [col.strip().replace('  ', ' ').replace(' ', '_').lower() for col in df.columns]\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "path = r'..\\Dataset\\Customer Churn.csv'\n",
    "df = read_file(path)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83f707",
   "metadata": {},
   "source": [
    "2.2 Display dataset data types and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aaa837",
   "metadata": {},
   "source": [
    "2.3 Display dataset shape and its statisical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cce58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc190a3",
   "metadata": {},
   "source": [
    "2.4. Detect Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a56f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, k=1.5):\n",
    "    nums = df.select_dtypes(include='number')\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for c in nums.columns:\n",
    "        #skip binary columns\n",
    "        if(nums[c].nunique() <= 2):\n",
    "            continue\n",
    "        \n",
    "        q1 = nums[c].quantile(0.25) #1st quartile\n",
    "        q3 = nums[c].quantile(0.75) #3rd quartile\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - k * iqr\n",
    "        upper = q3 + k * iqr\n",
    "        mask = (nums[c] < lower) | (nums[c] > upper)\n",
    "        outlier_info[c] = {\n",
    "            'count': int(mask.sum()),\n",
    "            'indices': nums.index[mask].tolist(),\n",
    "            'lower': float(lower),\n",
    "            'upper': float(upper)\n",
    "        }\n",
    "    return outlier_info\n",
    "\n",
    "outliers = detect_outliers_iqr(df)\n",
    "table_data = []\n",
    "print('\\nOutlier summary (IQR method):')\n",
    "for col, info in outliers.items():\n",
    "    if info['count'] > 0:\n",
    "    # Calculate percentage\n",
    "        perc = (info[\"count\"] / len(df)) * 100\n",
    "        \n",
    "        # Add a list (row) to our table_data\n",
    "        table_data.append([col, info[\"count\"], f\"{info['lower']:.3f}\",\n",
    "            f\"{info['upper']:.3f}\", f\"{perc:.2f}%\"])\n",
    "headers = [\"Column\", \"Outlier Count\", \"Lower Bound\", \"Upper Bound\", \"Percentage\"]\n",
    "print(tabulate(table_data, headers=headers))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f06022",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning \n",
    "This process take step before handling with outliers and data preprocessing.\n",
    "\n",
    "3.1 Remove Duplicate Row (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "print(f'\\n[Changes] Removed duplicate rows. New shape={df.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61f70f",
   "metadata": {},
   "source": [
    "3.2 Remove Redundant Groups\n",
    "\n",
    "`age_group` and `age` columns both have the same values but in different types, numeric and nominal respectively.\n",
    "`age_group` column is dropped to prevent biases when learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54814c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['age_group'])\n",
    "print(f'\\n[Changes] Dropped column: age_group due to redundancy. New shape={df.shape}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128d4f3",
   "metadata": {},
   "source": [
    "### 4. Data Preprocessing\n",
    "\n",
    "The dataset will follows the exact steps to avoid data leakage during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594373b",
   "metadata": {},
   "source": [
    "[Split] -> [Log] -> [Encode] -> [Fit] -> [Scale]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff70aca",
   "metadata": {},
   "source": [
    "#### 4.1 Split\n",
    "\n",
    "* Splitting the dataset into train, and test sets, and both input,X and output,Y\n",
    "* This function will reset the intialized index of X and Y dataset, and shuffle them before splitting to prevent **data leakage**\n",
    "\n",
    "@function `split_data()`:\n",
    "* params\n",
    "    * `X`, `y` as input and output\n",
    "    * `test_split` as testing split percentage ; default `0.2`\n",
    "    * `randomness` as random values for randomize dataset index before splitting ; default `None`\n",
    "* return\n",
    "    * `X_train` = Training set without `Churn`\n",
    "    * `Y_train` = `Churn` Training column\n",
    "    * `Y_train` = Testing set without `Churn`\n",
    "    * `Y_test` = `Churn` Testing column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63342842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_split=0.2, randomness=None):\n",
    "    # Set seed for reproducibility\n",
    "    if randomness is not None:\n",
    "        np.random.seed(randomness)\n",
    "    \n",
    "    # reset X and Y current index\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    # Identify unique classes and their indices (0 and 1)\n",
    "    unique_classes = np.unique(y)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        # Get indices of rows belonging to this class\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "\n",
    "        # Shuffle indices within this specific class\n",
    "        np.random.shuffle(cls_indices)\n",
    "\n",
    "        # Determine the split point\n",
    "        total_count = len(cls_indices)\n",
    "        test_count = int(total_count * test_split)\n",
    "        \n",
    "        # Split indices\n",
    "        cls_test = cls_indices[:test_count]\n",
    "        cls_train = cls_indices[test_count:]\n",
    "        \n",
    "        # Add to main lists\n",
    "        test_indices.extend(cls_test)\n",
    "        train_indices.extend(cls_train)\n",
    "        \n",
    "    # Shuffle the final combined indices so they aren't grouped by class\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    # Use .iloc for DataFrames to select the rows\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X = df.drop(columns=['churn'], axis=1)\n",
    "Y = df['churn']\n",
    "X_train,X_test,y_train,y_test = split_data(X,Y,test_split=0.2, randomness=42)\n",
    "print(f'[Changes] Successfully split data into Training and Testing.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6033f79",
   "metadata": {},
   "source": [
    "#### 4.2 Log Transformation\n",
    "\n",
    "* This will treat the outliers found before, compressing all the values within the dataset to **reduce** the skewness of the data\n",
    "* This step is to prevent unfair patterns.\n",
    "* Use `numpy.log(1 + x)`\n",
    "* log transformation on seperated train and test set to prevent **data leakage**\n",
    "\n",
    "@function `log_transformation()`:\n",
    "* params\n",
    "    * `df_train`, `df_test` as training and testing dataset\n",
    "    * `cols_log` as list of column to be log transformed\n",
    "* return\n",
    "    * `df_train` = train dataset that have been log transformed\n",
    "    * `df_test` = test dataset that have been log transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(df_train, df_test, cols_log: list):\n",
    "    for col in cols_log:\n",
    "        df_train[col] = np.log1p(df_train[col])\n",
    "        df_test[col] = np.log1p(df_test[col])\n",
    "    print(f'[Changes] Applied log transformation to selected columns.')\n",
    "    return df_train, df_test\n",
    "\n",
    "cols_to_log = [\n",
    "    'seconds_of_use',\n",
    "    'frequency_of_use',\n",
    "    'frequency_of_sms',\n",
    "    'distinct_called_numbers',\n",
    "    'call_failure',\n",
    "    'customer_value',\n",
    "    'charge_amount'\n",
    "]\n",
    "X_train, X_test = log_transformation(X_train,X_test,cols_to_log)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f2b00",
   "metadata": {},
   "source": [
    "#### 4.3 One-Hot Encoding\n",
    "\n",
    "* Convert categorical column into multiple, numeric dummy columns that values between `0` and `1`.\n",
    "* Example: `plan` have 3 categories  = `[cat1, cat2, cat3]`. It will split into dummy columns for each category, `plan_cat1`, `plan_cat2`, `plan_cat3` \n",
    "* use drop first column attribute to prevent **multicollinearity**\n",
    "\n",
    "@function `get_train_category()`:\n",
    "* params\n",
    "    * `df` as dataset\n",
    "    * `col_name` retrieve category from this column\n",
    "* return\n",
    "    * `list` = list of column name\n",
    "\n",
    "\n",
    "@function `one_hot_encoding()`:\n",
    "* params\n",
    "    * `df` as dataset\n",
    "    * `col_name` as column name that will apply one-hot encoding\n",
    "    * `categories` as category inside `col_name`\n",
    "    * `drop_first` drop first column to simplify the dummy column\n",
    "* return\n",
    "    * `converted_pd` = return the dataframe with the added dummy columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d582d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_categories(df, col_name):\n",
    "    return sorted(list(set(df[col_name].tolist())))\n",
    "\n",
    "def one_hot_encoding(df, col_name:str, categories, drop_first = True):\n",
    "    data = df[col_name].tolist()\n",
    "    \n",
    "    active_cats = categories[1:] if drop_first and len(categories) > 1 else categories\n",
    "    \n",
    "    encoded_mtx = []\n",
    "    for item in data:\n",
    "        row = [0] * len(active_cats)\n",
    "        if item in active_cats:\n",
    "            index = active_cats.index(item)\n",
    "            row[index] = 1\n",
    "        encoded_mtx.append(row)\n",
    "    \n",
    "    #rename column for one hot encoded column\n",
    "    new_cols = [f'{col_name}_{cat}' for cat in active_cats]\n",
    "    #convert back to dataframe\n",
    "    converted_pd  = pd.DataFrame(encoded_mtx, columns=new_cols,index=df.index)\n",
    "    return converted_pd\n",
    "\n",
    "train_encoded_parts = []\n",
    "test_encoded_parts = []\n",
    "\n",
    "categorical = ['complains', 'tariff_plan', 'status']\n",
    "for col in categorical:\n",
    "    train_categories = get_train_categories(X_train, col)\n",
    "    train_encoded_parts.append(one_hot_encoding(X_train, col, train_categories, drop_first=True))        \n",
    "    test_encoded_parts.append(one_hot_encoding(X_test, col, train_categories, drop_first=True))\n",
    "print(f'[Changes] Applied one hot encoding to categorical columns')\n",
    "\n",
    "#drop old column and join new columns\n",
    "X_train = X_train.drop(columns=categorical).join(train_encoded_parts)\n",
    "X_test = X_test.drop(columns=categorical).join(test_encoded_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15932e",
   "metadata": {},
   "source": [
    "#### 4.4 Scaling Feature\n",
    "\n",
    "* Transforms features to fit within a specific range, usually `0`, by subtracting the minimum value and dividing by the range `(max - min)`\n",
    "* Using `min()` and `max()` function to retrieve the minimum and maximum value in a column\n",
    "* Scales the transformation on training data, and transform on train and test dataset to prevent data leakage\n",
    "\n",
    "@function `get_scaling_params()`:\n",
    "* params\n",
    "    * `df_train` as training input dataset\n",
    "* return\n",
    "    * `param_dict` = a dictionary contains the minimum and maximum value for every key column\n",
    "\n",
    "@function `min_max_transform()`:\n",
    "* params\n",
    "    * `df` as input dataset\n",
    "    * `params` as dicitionary containing min and max value for each column\n",
    "* return\n",
    "    * `df_scaled` = scaled dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaling_params(df_train):\n",
    "    param_dict = {}\n",
    "    for col in df_train.columns:   \n",
    "        min_val = min(df_train[col])\n",
    "        max_val = max(df_train[col])\n",
    "        param_dict[col] = (min_val, max_val - min_val)\n",
    "    return param_dict\n",
    "\n",
    "#eg: min_max_transform(X_TEST, train_min_val, train_data_range)\n",
    "def min_max_transform(df, params: dict):\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    for col, (min_v, data_range) in params.items():\n",
    "        if data_range == 0:\n",
    "            df_scaled[col] = 0.0 #base case float number\n",
    "        else:\n",
    "            df_scaled[col] = (df[col] - min_v) / data_range\n",
    "            \n",
    "    return df_scaled\n",
    "\n",
    "X_train_scale_params = get_scaling_params(X_train)\n",
    "X_train_scaled = min_max_transform(X_train, X_train_scale_params)\n",
    "X_test_scaled = min_max_transform(X_test, X_train_scale_params)\n",
    "print(f'[Changes] Applied Min Max Scaler on numerical columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56428e0",
   "metadata": {},
   "source": [
    "### 5. Initialize the Model\n",
    "\n",
    "#### 5.1 Intialize Variables\n",
    "* `EPOCHS`      : Iteration counts\n",
    "* `PATIENCE`    : Hyperparameter stopping condition that wait for an improvement\n",
    "* `input_dim` = input layer size (nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1521273",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "PATIENCE = 100\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cf30f",
   "metadata": {},
   "source": [
    "@Class `NeuralNetwork`\n",
    "* A multi-layer perceptron (Backpropagation Neural Network) designed for binary classification.\n",
    "* He initialization for weight stability, gradient descent, and early stopping based on patience or error thresholds.\n",
    "\n",
    "@function `feedforward()`:\n",
    "* Feed the data forward, [Input] -> w1 -> [Hidden] -> w2 -> [Output]\n",
    "* Input to hidden layer will use **ReLU** activation function to handle nonlinear\n",
    "* Hidden layer to output layer will use **Sigmoid** activation function to **0 - 1** output\n",
    "\n",
    "@function `backpropagation()`:\n",
    "* Calculates the gradients of the loss function with respect to each weight and bias in the network.\n",
    "* Uses Gradient Clipping (max value of 5.0) to prevent exploding gradients and stability.\n",
    "* Updates weight1, weight2, bias1, and bias2 using the calculated deltas and the learning rate (`alpha`).\n",
    "\n",
    "@function `train()`\n",
    "* Manages the entire learning process over a set number of EPOCHS using  Gradient Descent.\n",
    "* Shuffles the training data every epoch to improve generalization and prevent the model from learning the order of the data.\n",
    "* Stopping Condition:\n",
    "    * `max_error`\n",
    "    * `patience`\n",
    "\n",
    "@function `calculate_accuracy()`\n",
    "* Computes the percentage of correct predictions by comparing predicted probabilities against true labels\n",
    "* 0.5 threshold to convert sigmoid probabilities into binary classes (0 or 1).\n",
    "\n",
    "@function `predict()`\n",
    "* Performs a final feedforward pass on unseen data to generate class predictions.\n",
    "* Flattens the final output and applies a boolean threshold (0.5) to return integer class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7816a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dimension, hidden1_nodes=16, hidden2_nodes=8, alpha=0.01):\n",
    "        #Step 0: Initialization\n",
    "        self.alpha = alpha\n",
    "        self.weight1 = np.random.randn(input_dimension, hidden1_nodes) * np.sqrt(2/input_dimension) #He Initializaion keeps Weight stable\n",
    "        self.weight2 = np.random.randn(hidden1_nodes, hidden2_nodes) * np.sqrt(2/hidden1_nodes)\n",
    "        self.weight3 = np.random.randn(hidden2_nodes, 1) * np.sqrt(2/hidden2_nodes)\n",
    "        self.bias1 = np.zeros((1, hidden1_nodes))\n",
    "        self.bias2 = np.zeros((1, hidden2_nodes))\n",
    "        self.bias3 = np.zeros((1, 1))\n",
    "        \n",
    "        self.train_loss, self.test_loss, self.train_acc, self.test_acc = [], [], [], []\n",
    "        self.history = {\n",
    "            'train_loss': self.train_loss,\n",
    "            'test_loss': self.test_loss,\n",
    "            'train_acc': self.train_acc,\n",
    "            'test_acc': self.test_acc\n",
    "        }    \n",
    "        \n",
    "    def feedforward(self,X):\n",
    "        # Step 1 : Calc Hidden Layer\n",
    "        self.hidden1_Z = X @ self.weight1 + self.bias1\n",
    "        self.hidden1_A = relu(self.hidden1_Z)\n",
    "        self.hidden2_Z = self.hidden1_A @ self.weight2 + self.bias2\n",
    "        self.hidden2_A = relu(self.hidden2_Z)\n",
    "        \n",
    "        # Step 2: Calc Output Layer\n",
    "        self.output_Z = self.hidden2_A @ self.weight3 + self.bias3\n",
    "        self.output_A= sigmoid(self.output_Z)\n",
    "        \n",
    "        return self.output_A\n",
    "    \n",
    "    def backpropagation(self, X, y, output):\n",
    "        size = y.shape[0] \n",
    "        \n",
    "        # Step 3: Calculate Error\n",
    "        d_output = (output - y)\n",
    "        \n",
    "        # Step 4: Calculate Output Error Gradient\n",
    "        d_weight3 = self.hidden2_A.T @ d_output / size\n",
    "        d_bias3 = np.sum(d_output, axis=0, keepdims=True) / size\n",
    "        \n",
    "        # Step 5: Calculate Hidden Error Gradient\n",
    "        d_hidden2_A = d_output @ self.weight3.T\n",
    "        d_hidden2_Z = d_hidden2_A * derivative_relu(self.hidden2_Z)\n",
    "        d_weight2 = self.hidden1_A.T @ d_hidden2_Z / size\n",
    "        d_bias2 = np.sum(d_hidden2_Z, axis=0, keepdims=True) / size\n",
    "        d_hidden1_A = d_hidden2_Z @ self.weight2.T\n",
    "        d_hidden1_Z = d_hidden1_A * derivative_relu(self.hidden1_Z)\n",
    "        d_weight1 = X.T @ d_hidden1_Z / size\n",
    "        d_bias1 = np.sum(d_hidden1_Z, axis=0, keepdims=True) / size\n",
    "        \n",
    "        # Step 6: Update Output Weight\n",
    "        self.weight3 -= self.alpha * d_weight3\n",
    "        self.bias3 -= self.alpha * d_bias3\n",
    "        \n",
    "        # Step 7 and 8: Update Hidden Weight\n",
    "        self.weight2 -= self.alpha * d_weight2\n",
    "        self.bias2 -= self.alpha * d_bias2\n",
    "        self.weight1 -= self.alpha * d_weight1\n",
    "        self.bias1 -= self.alpha * d_bias1\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred_prob):\n",
    "        # Threshold at 0.5 for binary classification\n",
    "        predictions = (y_pred_prob > 0.5).astype(int)\n",
    "        correct = np.sum(predictions == y_true)\n",
    "        return correct / len(y_true)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test):\n",
    "        epochs = EPOCHS\n",
    "        max_error = 0.01\n",
    "        best_loss = float('inf')\n",
    "        patience_count = 0\n",
    "        patience = PATIENCE\n",
    "        \n",
    "        X_tr = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "        y_tr = np.array(y_train).reshape(-1, 1)\n",
    "        X_te = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "        y_te = np.array(y_test).reshape(-1, 1)\n",
    "        \n",
    "        n_samples = X_tr.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Calculate metrics on full dataset\n",
    "            output_full = self.feedforward(X_tr)\n",
    "            self.backpropagation(X_tr, y_tr, output_full)\n",
    "            \n",
    "            train_loss = binary_cross_entropy(y_tr, output_full)\n",
    "            train_acc = np.mean((output_full > 0.5).astype(int) == y_tr) * 100\n",
    "            \n",
    "            output_test = self.feedforward(X_te)\n",
    "            test_loss = binary_cross_entropy(y_te, output_test)\n",
    "            test_acc = np.mean((output_test > 0.5).astype(int) == y_te) * 100\n",
    "            \n",
    "            self.train_loss.append(train_loss)\n",
    "            self.test_loss.append(test_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            self.test_acc.append(test_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "            \n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                patience_count = 0\n",
    "            else:\n",
    "                patience_count += 1\n",
    "            \n",
    "            if patience_count >= patience:\n",
    "                print(f'[Training Stopped] Patience {patience} reached')\n",
    "                break\n",
    "            \n",
    "            if train_loss <= max_error:\n",
    "                print(f'[Training Stopped] Max error {max_error} reached')\n",
    "                break\n",
    "            \n",
    "        return self.history\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_vals = X.values if isinstance(X, pd.DataFrame) else X        \n",
    "        h1_Z = X_vals @ self.weight1 + self.bias1\n",
    "        h1_A = relu(h1_Z)\n",
    "        h2_Z = h1_A @ self.weight2 + self.bias2\n",
    "        h2_A = relu(h2_Z)\n",
    "        o_Z = h2_A @ self.weight3 + self.bias3\n",
    "        probs = sigmoid(o_Z)\n",
    "        return (probs > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9581b",
   "metadata": {},
   "source": [
    "5.2 Initialize, Train and Predict\n",
    "\n",
    "* Input Layer : 12 nodes\n",
    "* Hidden Layer 1 : 16 nodes\n",
    "* Hidden Layer 2 : 8 nodes\n",
    "* Learning Rate : 0.01\n",
    "* Epochs : 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn = NeuralNetwork(input_dimension= input_dim, alpha=0.01)\n",
    "history = nn.train(X_train_scaled, y_train, X_test_scaled, y_test)    \n",
    "y_hat = nn.predict(X_test_scaled)\n",
    "\n",
    "print('Model finished training and validation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca9c9f",
   "metadata": {},
   "source": [
    "@function `evaluate_model_performance()`:\n",
    "\n",
    "* Generates a comprehensive visual report of the model's training history and final classification performance in a 3-row vertical layout.\n",
    "\n",
    "* **Loss Curve**: Visualizes the relationship between training and validation loss across epochs to detect overfitting or convergence issues.\n",
    "\n",
    "* **Accuracy Curve** : Tracks the percentage of correct predictions over time for both the training and test sets.\n",
    "\n",
    "* **Confusion Matrix** : Calculate the classification metrics (TP, TN, FP, FN) and displays them as a Seaborn heatmap for error analysis.\n",
    "\n",
    "* params\n",
    "    * `history`: A dictionary containing the logged lists of loss and accuracy values.\n",
    "    * `y_true` : The truth labels from the test dataset.\n",
    "    * `y_pred` : The binary predictions (0 or 1) generated by the model.\n",
    "    * `title` : string used as the main heading for the entire figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(history:dict, y_true:list, y_pred:list, title:str):\n",
    "    # Create a figure within 3 rows\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(6, 20))\n",
    "    fig.suptitle(f'Model Evaluation: {title}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # --- 1. Loss Curve Plot ---\n",
    "    train_loss = history['train_loss']\n",
    "    val_loss = history['test_loss']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    axes[0].plot(epochs, train_loss, label='Training Loss', color='blue', lw=1.5, marker='x', ms=4)\n",
    "    axes[0].plot(epochs, val_loss, label='Validation Loss', color='orange', lw=1.5, marker='x', ms=4)\n",
    "    axes[0].set_title('Loss Curve (Learning Curve)')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # --- 2. Accuracy Plot ---\n",
    "    train_acc = history['train_acc']\n",
    "    val_acc = history['test_acc']\n",
    "    axes[1].plot(epochs, train_acc, label='Training Accuracy', color='blue', lw=1.5, marker='x', ms=4)\n",
    "    axes[1].plot(epochs, val_acc, label='Validation Accuracy', color='orange', lw=1.5, marker='x', ms=4)\n",
    "    axes[1].set_title('Accuracy Over Epochs')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # --- 3. Confusion Matrix ---\n",
    "    actual = np.array(y_true).flatten()\n",
    "    predicted = np.array(y_pred).flatten()\n",
    "    \n",
    "    tp = np.sum((actual == 1) & (predicted == 1))\n",
    "    tn = np.sum((actual == 0) & (predicted == 0))\n",
    "    fp = np.sum((actual == 0) & (predicted == 1))\n",
    "    fn = np.sum((actual == 1) & (predicted == 0))\n",
    "    cm = np.array([[tn, fp], [fn, tp]])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Stay', 'Churn'], \n",
    "            yticklabels=['Stay', 'Churn'],\n",
    "            ax=axes[2])\n",
    "    axes[2].set_title('Confusion Matrix: Churn Prediction')\n",
    "    axes[2].set_ylabel('Actual')\n",
    "    axes[2].set_xlabel('Predicted')\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "evaluate_model_performance(history, y_test, y_hat, title='Model 2: 2 Hidden Layers')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
